%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Short Sectioned Assignment
% LaTeX Template
% Version 1.0 (5/5/12)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% Original author:
% Frits Wenneker (http://www.howtotex.com)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[paper=a4, fontsize=11pt]{scrartcl} % A4 paper and 11pt font size
\usepackage[top=1in, bottom=1.5in, left=1in, right=1in]{geometry}
\usepackage{fancyhdr} % Required for custom headers
\usepackage{lastpage} % Required to determine the last page for the footer
\usepackage{extramarks} % Required for headers and footers
\usepackage[usenames,dvipsnames]{color} % Required for custom colors
\usepackage{graphicx} % Required to insert images
\usepackage{listings} % Required for insertion of code
\usepackage{courier} % Required for the courier font
\usepackage{amsmath}
\usepackage[super]{nth}
\usepackage{booktabs}
\usepackage[usenames,dvipsnames]{xcolor}
\usepackage{tcolorbox}
\usepackage{tabularx}
\usepackage{array}
\usepackage{colortbl}

%\usepackage[T1]{fontenc} % Use 8-bit encoding that has 256 glyphs
%\usepackage{fourier} % Use the Adobe Utopia font for the document - comment this line to return to the LaTeX default
\usepackage[english]{babel} % English language/hyphenation
\usepackage{amsmath,amsfonts,amsthm} % Math packages
\usepackage{graphicx}

%\usepackage{lipsum} % Used for inserting dummy 'Lorem ipsum' text into the template

\usepackage{sectsty} % Allows customizing section commands
\allsectionsfont{\centering \normalfont\scshape} % Make all sections centered, the default font and small caps
\usepackage{listings} % Required for insertion of code
\usepackage{hyperref}
\hypersetup{
  colorlinks   = true, %Colours links instead of ugly boxes
  urlcolor     = blue, %Colour for external hyperlinks
  linkcolor    = blue, %Colour of internal links
  citecolor   = red %Colour of citations
}

\usepackage{fancyhdr} % Custom headers and footers
\pagestyle{fancyplain} % Makes all pages in the document conform to the custom headers and footers
\fancyhead{} % No page header - if you want one, create it in the same way as the footers below
\fancyfoot[L]{} % Empty left footer
\fancyfoot[C]{} % Empty center footer
\fancyfoot[R]{\thepage} % Page numbering for right footer
\renewcommand{\headrulewidth}{0pt} % Remove header underlines
\renewcommand{\footrulewidth}{0pt} % Remove footer underlines
\setlength{\headheight}{13.6pt} % Customize the height of the header
\newcommand{\ts}{\textsuperscript}

\numberwithin{equation}{section} % Number equations within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)
\numberwithin{figure}{section} % Number figures within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)
\numberwithin{table}{section} % Number tables within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)

\setlength\parindent{0pt} % Removes all indentation from paragraphs - comment this line for an assignment with lots of text

% Default fixed font does not support bold face
\DeclareFixedFont{\ttb}{T1}{txtt}{bx}{n}{8} % for bold
\DeclareFixedFont{\ttm}{T1}{txtt}{m}{n}{8}  % for normal

% Custom colors
\usepackage{color}
\definecolor{mygreen}{rgb}{0,0.6,0}
\definecolor{mygray}{rgb}{0.5,0.5,0.5}
\definecolor{mymauve}{rgb}{0.58,0,0.82}

\usepackage{listings}

% Python style for highlighting
\newcommand\pythonstyle{\lstset{
language=Python,
  backgroundcolor=\color{white},   % choose the background color
  basicstyle=\footnotesize,        % size of fonts used for the code
  %breaklines=true,                 % automatic line breaking only at whitespace
  frame=tb,
  captionpos=b,                    % sets the caption-position to bottom
  commentstyle=\color{mygreen},    % comment style
  escapeinside={\%*}{*)},          % if you want to add LaTeX within your code
  keywordstyle=\color{blue},       % keyword style
  stringstyle=\color{mymauve},     % string literal style
}}


% Python environment
\lstnewenvironment{python}[1][]
{
\pythonstyle
\lstset{#1}
}
{}

% Python for external files
\newcommand\pythonexternal[2][]{{
\pythonstyle
\lstinputlisting[#1]{#2}}}

% Python for inline
\newcommand\pythoninline[1]{{\pythonstyle\lstinline!#1!}}

\usepackage{adjustbox}

%----------------------------------------------------------------------------------------
%	TITLE SECTION
%----------------------------------------------------------------------------------------

\newcommand{\horrule}[1]{\rule{\linewidth}{#1}} % Create horizontal rule command with 1 argument of height

\title{	
\normalfont \normalsize
\textsc{University of Notre Dame, Computer Science and Engineering} \\ [25pt] % Your university, school and/or department name(s)
\horrule{0.5pt} \\[0.4cm] % Thin top horizontal rule
\huge CSE 40647/60647  Data Mining \textemdash~Assignment 2\\Due Date: March \nth{3}, 2014 at 11:59pm ET\\ % The assignment title
\horrule{2pt} \\[0.5cm] % Thick bottom horizontal rule
}

\author{\textbf{Association Analysis, Data Preprocessing, and Clustering }} % Your name

\date{\normalsize{February 23, 2014}} % Today's date or a custom date

\begin{document}
\maketitle % Print the title

%----------------------------------------------------------------------------------------
%	PROBLEM 1
%----------------------------------------------------------------------------------------

This assignment will require you to implement and interpret some of the data processing concepts that were introduced in class, such as association analysis, dimensionality reduction, and clustering.  \textbf{An IPython Notebook with predefined functions to assist you with this assignment is available \href{http://nbviewer.ipython.org/github/cse40647/cse40647/blob/sp.14/assignment2/assignment2.ipynb}{here}. Additionally, the datasets used in this assignment can be downloaded \href{https://github.com/cse40647/cse40647/blob/sp.14/assignment2/bakery.zip}{here}, \href{https://github.com/cse40647/cse40647/blob/sp.14/assignment2/campus.zip}{here}, and \href{https://github.com/cse40647/cse40647/blob/sp.14/assignment2/cute_kittens.zip}{here}.} Keep in mind that the main objective of this assignment is to highlight the insights that we can derive from applying these techniques---the coding aspect is secondary. Accordingly, you are welcome to consult any online documentation and/or code that has been posted to the course website, so long as all references and sources are properly cited. You are also encouraged to use code libraries, so long as you acknowledge any source code that was not written by you by mentioning the original author(s) directly in your source code (comment or header).

%\linebreak
\vspace{10pt}

\textbf{You are expected to submit a single IPython Notebook file following the same instructions and naming convention described in Assignment 0. Answers to the conceptual questions can be embedded in the Notebook as \textit{markdown} cells, and you may use \textit{heading} cells to further organize your document.}

\newpage

\section{Association Rules (30 points)}

\textbf{The Data}\\
For this portion of the assignment you will be using the \textit{Extended Bakery dataset}, which describes transactions from a chain of bakery shops that sell a variety of drinks and baked goods. More information about the original unmodified dataset can be found \href{https://wiki.csc.calpoly.edu/datasets/wiki/ExtendedBakery}{here}.

\vspace{6pt}

You may download the dataset \href{https://github.com/cse40647/cse40647/blob/sp.14/assignment2/bakery.zip}{here}.

\vspace{6pt}

This particular dataset is made up of smaller subsets that contain 1,000, 5,000, 20,000 and 75,000 transactions each. Furthermore, each of these subsets is given in two different formats. You are free to use either one when writing your solutions. Below is a description of each:

\begin{enumerate}
\item Sparse Vector Format: \texttt{XXX-out1.csv} files. Each line of the file has the following format:
\begin{itemize}
\item First column: transaction ID
\item Subsequent columns: list of purchased goods (represented by their ID code)
\end{itemize}
\textbf{Example:}\\
3,0,2,4,6\\
Transaction 3 contained items 0, 2, 4, 6
\item Full Binary Vector Format: \texttt{XXX-out2.csv} files. Each line has the following format:
\begin{itemize}
\item First column: transaction ID
\item 50 subsequent columns: A sequence of binary values that indicate if item \textit{i} (represented in column \textit{i}) has been purchased in that transaction.
\end{itemize}
\textbf{Example:}\\
3,0,0,1,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0\\
Transaction 3 contained items 0, 2, 4, 6
\end{enumerate}

The sparse vector format can be easily read into a list of transactions in Python by using the following code snippet:

\vspace{6pt}

\begin{adjustbox}{minipage=0.8\textwidth,margin=0pt \smallskipamount,center}
\begin{python}
T = [] # list of transactions
infile = open('/path/to/file/file.csv','r')
for line in infile:
      # Append new transaction (skipping \n and transaction number)
      T.append(line.strip().split(',')[1:])
\end{python}
\end{adjustbox}

\vspace{8pt}

\textbf{The Idea:  Association Rule Learning}\\
Given the database of transactions, where each transaction is a list of items, find rules that associate the presence of one set of items with that of another set of items. Ideally, we only want to find rules that are substantiated by the data; we want to avoid spurious associations.

\vspace{8pt}

\textbf{What to Do}\\
Table 1.1 provides a map between the product IDs and the actual items. Your task is to find the association rules that exceed specific values of \textit{minimum support} and \textit{minimum confidence}. You are free to experiment with different values until you find something that produces meaningful/interesting results.

\vspace{6pt}

Recall from our class discussions that this process requires two steps, namely finding the \textit{frequent itemsets} and discovering the strong \textit{association rules} within these. To do so, you may re-use some of the code that we provided for Apriori\footnote{\href{http://nbviewer.ipython.org/github/cse40647/cse40647/blob/sp.14/10\%20-\%20Apriori.ipynb}{http://nbviewer.ipython.org/github/cse40647/cse40647/blob/sp.14/10\%20-\%20Apriori.ipynb}}, write your own code, or use any Python modules that offer that functionality. Whatever option you choose to follow, be sure that your output is formatted in a clear and legible manner.

\vspace{8pt}

\textbf{What to Provide}\\
Your output should contain the following:
\begin{itemize}
\item For each frequent itemset:
\begin{enumerate}
	\item All items in it, described by the product names (the \texttt{products.csv} file may be helpful on mapping IDs $\rightarrow$ names).
    \item The support of the itemset.
\end{enumerate}
\item For each rule:
\begin{enumerate}
	\item All items on the left side of the rule.
    \item All items on the right side of the rule.
    \item The support of the rule.
    \item The confidence of the rule.
\end{enumerate}
\end{itemize}
Given this output, respond to the following question:
\begin{enumerate}
	\item Compare the rules that you obtained for each different subset (1,000\textendash75,000 transactions). How does the number of transactions affect the results you observed?
\end{enumerate}

\vspace{64pt}

\begin{table}[h]
\begingroup
\fontsize{6pt}{10pt}\selectfont
\begin{minipage}[t]{0.18\linewidth}
\centering
	\begin{tabular}[t]{ll}
    \toprule
\textbf{ID} & \textbf{Product} \\
    \midrule
\textbf{0} & Chocolate Cake \\
\textbf{1} & Lemon Cake \\
\textbf{2} & Casino Cake \\
\textbf{3} & Opera Cake \\
\textbf{4} & Strawberry Cake \\
\textbf{5} & Truffle Cake \\
\textbf{6} & Chocolate Eclair \\
\textbf{7} & Coffee Eclair \\
\textbf{8} & Vanilla Eclair \\
\textbf{9} & Napoleon Cake \\
    \bottomrule
    \end{tabular}%
\end{minipage}
\begin{minipage}[t]{0.18\linewidth}
\centering
    \begin{tabular}[t]{ll}
    \toprule
\textbf{ID} & \textbf{Product} \\
    \midrule
\textbf{10} & Almond Tart \\
\textbf{11} & Apple Pie \\
\textbf{12} & Apple Tart \\
\textbf{13} & Apricot Tart \\
\textbf{14} & Berry Tart \\
\textbf{15} & Blackberry Tart \\
\textbf{16} & Blueberry Tart \\
\textbf{17} & Chocolate Tart \\
\textbf{18} & Cherry Tart \\
\textbf{19} & Lemon Tart \\
\bottomrule
    \end{tabular}%
\end{minipage}
\begin{minipage}[t]{0.19\linewidth}
\centering
    \begin{tabular}[t]{ll}
    \toprule
\textbf{ID} & \textbf{Product} \\
    \midrule
\textbf{20} & Pecan Tart \\
\textbf{21} & Ganache Cookie \\
\textbf{22} & Gongolais Cookie \\
\textbf{23} & Raspberry Cookie \\
\textbf{24} & Lemon Cookie \\
\textbf{25} & Chocolate Meringue \\
\textbf{26} & Vanilla Meringue \\
\textbf{27} & Marzipan Cookie \\
\textbf{28} & Tuile Cookie \\
\textbf{29} & Walnut Cookie \\
    \bottomrule
    \end{tabular}%
\end{minipage}
\begin{minipage}[t]{0.19\linewidth}
\centering
    \begin{tabular}[t]{ll}
    \toprule
\textbf{ID} & \textbf{Product} \\
    \midrule
\textbf{30} & Almond Croissant \\
\textbf{31} & Apple Croissant \\
\textbf{32} & Apricot Croissant \\
\textbf{33} & Cheese Croissant \\
\textbf{34} & Chocolate Croissant \\
\textbf{35} & Apricot Danish \\
\textbf{36} & Apple Danish \\
\textbf{37} & Almond Twist \\
\textbf{38} & Almond Bear Claw \\
\textbf{39} & Blueberry Danish \\
    \bottomrule
    \end{tabular}%
\end{minipage}
\begin{minipage}[t]{0.19\linewidth}
\centering
    \begin{tabular}[t]{ll}
    \toprule
\textbf{ID} & \textbf{Product} \\
    \midrule
\textbf{40} & Lemonade \\
\textbf{41} & Raspberry Lemonade \\
\textbf{42} & Orange Juice \\
\textbf{43} & Green Tea \\
\textbf{44} & Bottled Water \\
\textbf{45} & Hot Coffee \\
\textbf{46} & Chocolate Coffee \\
\textbf{47} & Vanilla Frappuccino \\
\textbf{48} & Cherry Soda \\
\textbf{49} & Single Espresso \\
    \bottomrule
    \end{tabular}%
\end{minipage}
\endgroup
\caption{Products in the dataset and their corresponding codes.}
\end{table}

\newpage
\section{Data Preprocessing (30 points)}

\textbf{The Data}\\
For this portion of the assignment you will be using a set of images of the University of Notre Dame campus. The dataset consists of 30 color images capturing the campus during the spring, fall, and winter (10 images for each season). Each image consists of 62,500 pixels (250x250 pixels). Each pixel is defined by three color values varying between 0 and 255 that specify the degree or red, green, and blue present at that point.

\vspace{6pt}

You may download the dataset \href{https://github.com/cse40647/cse40647/blob/sp.14/assignment2/campus.zip}{here}.

\vspace{8pt}

\textbf{The Idea:  PCA with Image Histograms}\\
Your objective here will be to perform dimensionality reduction on the dataset to view the similarity of the images within some lower-dimensional (feature) space. Specifically, we would like to perform a transformation on the images so that they can be viewed in a 2-dimensional space, which we can then visualize via a plot. This can be accomplished by performing principal component analysis (PCA) on some property of the images and retaining only the top 2 principal components. Ideally, we want to apply PCA to a property of the images that can be used to capture some notion of the ``similarity'' between images.

\vspace{6pt}

Intuitively, since we typically view an image as a collection of pixels, we might consider performing PCA on the set of pixels, reducing an image from a collection of 62,500 pixel values to a new 2-dimensional space. In other words, we would consider each pixel as a feature of an image, and try to transform an image into a new set of 2 features. Ideally, images that have a similar set of pixels should also be similar in this 2-dimensional space (i.e., they should share similar feature values). However, due to the magnitude of this reduction, a significant amount of information would likely be lost in the transformation, meaning that pictures with similar pixel values many not appear similar in the the new 2-dimensional space.

\vspace{6pt}

A better approach might be to reduce the histogram of color values. Each color has 256 possible values, resulting in a histogram of 768 (256*3) color values distributed over the entire range of 65,000 pixels. Each value of the histogram is the number of pixels in the image with the corresponding color value. Here, we would consider each histogram value as a feature of an image. By performing PCA on an image histogram, we are only reducing an image from a set of 768 unique histogram values (rather than the 65,000 unique pixel values) to a new 2-dimensional space. As a result of this transformation, images that have a similar histogram should also be similar in this 2-dimensional space.

\vspace{8pt}

\textbf{What to Do}\\
The IPython Notebook provided with this assignment includes functions to compute the histograms and plot the images within the transformed (2-dimensional) space (\texttt{load\char`_images} and \texttt{plot\char`_image\char`_space}, respectively). There are also functions to generate and plot the color palettes associated with each image (\texttt{cluster\char`_image\char`_colors} and \texttt{plot\char`_color\char`_palette}, respectively); the palettes are generated via (\textit{k}-means) clustering of the pixel color values, and may be investigated at your own leisure---they are not needed to complete the assignment.

\vspace{6pt}

The images can be loaded and the histograms generated by running the following code snippet (which imports pandas as \texttt{pd}). Please ensure that the directory provided to the \texttt{load\char`_images} function is correct. For example, if you have placed all the images in your base IPython Notebook directory in a folder labeled \texttt{images}, with the campus images in a subfolder labeled \texttt{campus}, then the (relative) path to the campus images would be \texttt{images/campus}.

\vspace{6pt}

\begin{adjustbox}{minipage=0.8\textwidth,margin=0pt \smallskipamount,center}
\begin{python}
import pandas as pd
# Load images and generate color histograms
images = load_images('/path/to/campus_images')
X = pd.DataFrame([im.histogram() for im in images])
\end{python}
\end{adjustbox}

\vspace{6pt}

Once you have a DataFrame of image histograms, the PCA transformation of this data can be computed and plotted in Python by running the following code snippet (which imports \texttt{decomposition} from scikit-learn):

\vspace{6pt}

\begin{adjustbox}{minipage=0.8\textwidth,margin=0pt \smallskipamount,center}
\begin{python}
from sklearn import decomposition
# Generate PCA transformation and plot the results
estimator = decomposition.PCA(n_components=2)
X_proj = estimator.fit_transform(X)
plot_image_space(images, X_proj, 'title_of_plot')
\end{python}
\end{adjustbox}

\vspace{6pt}

The plot generated by this code snippet will display the images according to the top 2 principal components (or eigenvectors) found by the PCA transformation of the image histogram. %The closer any two images are in this space, the more similar their color histograms should be.

\vspace{8pt}

\textbf{What to Provide}\\
Your output should contain the following:
\begin{itemize}
\item The PCA projection of the image color histograms in 2 dimensions. Using the provided \texttt{plot\char`_image\char`_space} function, this should be displayed as thumbnail images distributed within a 2-dimensional plot.
\end{itemize}
Given this output, respond to the following questions:
\begin{enumerate}
\item What does it mean for two images to be close together in this plot? What does it mean for two images to be far apart?
\item Do images corresponding to one of the seasons tend to group together more closely than others? Why might this be the case?
\end{enumerate}


\newpage
\section{Clustering (40 points)}

\textbf{The Data}\\
For this portion of the assignment you will be using the \textit{Iris flower dataset}. The dataset consists of 50 samples from each of three species of Iris, with four features measured from each sample. scikit-learn provides a function to load the dataset (no download required).

\vspace{6pt}

\textbf{The Idea:  Choosing \textit{k} for \textit{k}-means}\\
Your objective here will be to assess the performance of \textit{k}-means clustering on the Iris dataset. Recall that the number of clusters, \textit{k}, is an input parameter to the \textit{k}-means algorithm. A variety of measurements are available for estimating the optimal value of \textit{k}. For this assignment, you will look at the sum of squared deviation (SSQ) and the gap statistic. Both of these criteria make use of the intuition that \textit{k}-means tries to minimize variance (the distance or deviation of each point from each of the \textit{k} clusters) by iteratively assigning points to their nearest clusters.

\vspace{8pt}

\textbf{Choosing \textit{k} with SSQ}\\
The SSQ criterion is a direct application of the intuition that \textit{k}-means tries to minimize variance. Recall that the SSQ criterion sweeps over a range of possible \textit{k} values, with each value of \textit{k} associated with a degree of deviation (the distance of each point from each of the \textit{k} clusters). These deviations can be squared and summed to arrive at the ``sum of squared deviation'' (SSQ) for each value of \textit{k}. Larger values of \textit{k} are expected to continue to reduce the SSQ (because there are more clusters for points to be near, reducing their deviation). However, one could expect a leveling-off in the SSQ once the value of \textit{k} exceeds the true number of clusters, as this would result in true clusters (that is, clusters actually present in the data) being separated. If, then, one plots the SSQ over a range of \textit{k} values, this leveling-off point may produce a noticeable ``elbow'' in the plot. By this criterion, the estimated optimal value of \textit{k} is that which occurs at this elbow point. While simple, the difficulty with this criterion is that often the elbow point is not distinctive or well-defined.

\vspace{8pt}

\textbf{Choosing \textit{k} with the Gap Statistic}\\
The gap statistic provides a criterion that produces a quantifiable estimate of the optimal value of \textit{k} over a range of possible \textit{k} values. The intuition here is that there is an expected degree of deviation associated with clustering any given dataset. We want the number of clusters, \textit{k}, that displays the largest ``gap'' between the deviation we expect, given the dataset and the number of clusters, and the deviation we estimate or observe. Thus, rather than simply considering estimated deviation by itself, we can standardize the estimated deviation for a possible value of \textit{k} by comparing it with the expected deviation under an appropriate null reference distribution of the data (e.g., a uniform distribution). This difference or gap between the expected deviation and the estimated deviation is termed the gap statistic. Maximizing this gap statistic then corresponds to minimizing the estimated deviation relative to what would be expected. To ensure that we do not needlessly posit additional clusters (i.e., larger values of \textit{k}), we only consider the value \textit{k+1} if its gap statistic (minus any measurement error) is higher than that for \textit{k}. By this criterion, the lowest value of \textit{k} with a corresponding gap statistic higher than or equal to the gap statistic of \textit{k+1} is the estimated optimal value of \textit{k}.

\vspace{8pt}

\textbf{What to Do}\\
The provided assignment IPython Notebook includes functions to compute and plot the gap statistic (\texttt{gap\char`_statistics} and \texttt{plot\char`_gap\char`_statistics}) and the sum of squared deviation (\texttt{ssq\char`_statistics} and \texttt{plot\char`_ssq\char`_statistics}).

\vspace{6pt}

The Iris flower dataset can be loaded in Python by using the following code snippet (with \texttt{datasets} imported from scikit-learn):

\vspace{6pt}

\begin{adjustbox}{minipage=0.8\textwidth,margin=0pt \smallskipamount,center}
\begin{python}
from sklearn import datasets
# Load the Iris flower dataset
iris = datasets.load_iris()
data = iris.data
\end{python}
\end{adjustbox}

\vspace{6pt}

Then the sum of squared deviations (SSQ) can be easily run and the results plotted by using the following code snippet:

\vspace{6pt}

\begin{adjustbox}{minipage=0.8\textwidth,margin=0pt \smallskipamount,center}
\begin{python}
# Generate and plot the SSQ statistics
ssqs = ssq_statistics(data, ks=range(2,11+1))
plot_ssq_statistics(ssqs)
\end{python}
\end{adjustbox}

\vspace{6pt}

Similarly, the gap statistic can be easily run and the results plotted by using the following code snippet:

\vspace{6pt}

\begin{adjustbox}{minipage=0.8\textwidth,margin=0pt \smallskipamount,center}
\begin{python}
# Generate and plot the gap statistics
gaps, errs, difs = gap_statistics(data, nrefs=20, ks=range(2,11+1))
plot_gap_statistics(gaps, errs, difs)
\end{python}
\end{adjustbox}

\vspace{6pt}

Both the SSQ and gap statistic code snippets require a variable \texttt{ks}, which defines the range of \textit{k} values (the ``\textit{k}s'') to evaluate. For example, if you would like to evaluate \textit{k} values between 2 an 11 (inclusive), you could set \texttt{ks} as \texttt{range(2,11+1)}.

\vspace{6pt}

The function \texttt{plot\char`_gap\char`_statistics} generates two plots. The first plot simply displays the gap statistic for each \textit{k} value evaluated. The second plot displays the difference between the gap statistic for value \textit{k} and that computed for value \textit{k+1}. On the second plot, the first non-negative value, or gap difference, is the optimal number of clusters estimated by the gap statistic criterion.

\vspace{8pt}

\textbf{What to Provide}\\
Your output should contain the following:
\begin{itemize}
\item The SSQs computed for \textit{k} values between 1 and 10 (inclusive). There should be one plot corresponding to the SSQs.
\item The gap statistics computed for \textit{k} values between 1 and 10 (inclusive). There should be two plots corresponding to the gap statistics.
\end{itemize}
Given this output, respond to the following questions:
\begin{enumerate}
\item Where did you estimate the elbow point to be (between what values of \textit{k})? What value of \textit{k} was typically estimated as optimal by the gap statistic? To adequately answer this question, consider generating both measures several times, as there may be some amount of variation in the value of \textit{k} that they each estimate as optimal.
\item How close are the estimates generated by the elbow point and gap statistic to the number of species of Iris represented in the dataset?
\item Assuming we are trying to generate one cluster for each Iris species represented in the dataset, does one measure seem to be a consistently better criterion for choosing the value of \textit{k} than the other? Why or why not?
\end{enumerate}


\newpage
\section{Graduate Student Portion\\(20 points / +10 points for undergraduates)}

\textbf{The Data}\\
We know what can improve this assignment:  cute kittens. So for this portion of the assignment you will be using a set of (cute) kitten images. The dataset consists of 25 color images, each preprocessed so that the kittens' faces are similar in size and orientation. Each image consists of 62,500 pixels (250x250 pixels).

\vspace{6pt}

You may download the dataset \href{https://github.com/cse40647/cse40647/blob/sp.14/assignment2/campus.zip}{here}.

\vspace{8pt}

\textbf{The Idea:  Dimensionality Reduction in Face Recognition}\\
Your job, if you accept it (and if you're a graduate student, you must), is to generate face clusters (via \textit{k}-means clustering) and eigenfaces (via PCA) from this set of kitten face images.

\vspace{6pt}

\textbf{Generating Clusters of Faces}\\
Intuitively, if we wanted to reduce the number of images in a dataset, we could try to cluster these images into groups. If each group is defined by a cluster centroid (that is, an image that represents all members of the cluster) as in \textit{k}-means clustering, then we could simply retain these centroid images and discard the remaining ones. In this way, we are considering each image as  a dimension, and reducing the dimensionality of the set of images, much to the same effect as PCA. A drawback of this method, however, is that there may not be any natural clustering of the images, resulting in a potentially significant loss of information if we retain only the centroid images.

\vspace{6pt}

\textbf{Generating Eigenfaces}\\
Recall that the Eigenface approach to facial recognition is the use of PCA to reduce a collection of face images to a smaller set of reference images. The term typically refers to the use of this approach on human faces, but the same concept is applicable to kitten faces as well. The intuition here is that the eigenfaces will constitute a set of ``standardized face ingredients,'' which may be used to match subsequent face images. We are considering each image as a dimension, and reducing the dimensionality of the set of images.

\vspace{6pt}

Note that both the face clustering and Eigenface approaches perform dimensionality reduction on the set of images (that is, we reduce the number of images, where each image is a dimension). This contrasts with performing dimensionality reduction on individual images by reducing the set of pixels to a smaller set of component features (where each pixel is a dimension).

\vspace{8pt}

\textbf{What to Do}\\
The provided assignment IPython Notebook includes functions to load and plot the (kitten) images (\texttt{load\char`_images} and \texttt{plot\char`_gallery}, respectively).

\vspace{6pt}

The kitten images can be easily loaded in Python by using the following code snippet, which makes use of our provided \texttt{load\char`_images} function (and assumes that NumPy has been imported as \texttt{np}). Please ensure that the directory provided to the \texttt{load\char`_images} function is correct. For example, if you have placed all the images in your base IPython Notebook directory in a folder labeled \texttt{images}, with the kitten images in a subfolder labeled \texttt{cute\char`_kittens}, then the (relative) path to the kitten images would be \texttt{images/cute\char`_kittens}.

\vspace{6pt}

\begin{adjustbox}{minipage=0.8\textwidth,margin=0pt \smallskipamount,center}
\begin{python}
# Load an array of kitten images
kittens = load_images('/path/to/kitten_images', grayscale=True)
kittens = np.array(kittens) # transform to a NumPy array
n_samples, n_features = kittens.shape # number of rows and columns
image_shape = (100, 100) # the image dimensions (width and height)
\end{python}
\end{adjustbox}

\vspace{6pt}

Before performing dimensionality reduction (our objective in generating clusters and eigenfaces), some additional data preprocessing is needed. Generally, dimensionality reduction techniques require centering the data globally (i.e., subtracting the mean from each data point for each feature), because otherwise they may capture spurious fluctuations in the data. Though not strictly needed, we may also center the data locally (i.e., subtracting the mean from each data point for each image) to enhance the visualization of the images.

\vspace{6pt}

The following code snippet centers the kitten data and displays the first few images in the centered dataset by making use of our provided \texttt{plot\char`_gallery} function. The snippet assumes that the number of retained components  (clusters or eigenfaces) has been defined as \texttt{n\char`_components} (an integer) and that the number of columns and rows of images to display has been defined as  \texttt{n\char`_col} and  \texttt{n\char`_row} (also integers), respectively. Note that \texttt{n\char`_components} must equal the product of \texttt{n\char`_col} and \texttt{n\char`_row}, or you may otherwise receive an error.

\vspace{6pt}

\begin{adjustbox}{minipage=0.8\textwidth,margin=0pt \smallskipamount,center}
\begin{python}
# Global centering
kittens_centered = kittens - kittens.mean(axis=0)
# Local centering
kittens_centered -= kittens_centered.mean(axis=1).reshape(n_samples,-1)
# Plot the first few centered kitten images
assert n_components == n_col*n_row
plot_gallery("title_of_plot", kittens[:n_components],n_col, n_row)
\end{python}
\end{adjustbox}

\vspace{6pt}

The following code snippet performs \textit{k}-means clustering on the centered kitten data, thus generating face clusters, and plots the results (with the number of clusters defined as \texttt{n\char`_components} and the number of columns and rows of images to display defined as  \texttt{n\char`_col} and  \texttt{n\char`_row}, respectively):

\vspace{6pt}

\begin{adjustbox}{minipage=0.8\textwidth,margin=0pt \smallskipamount,center}
\begin{python}
# Compute k-means clustering on the dataset
estimator = sklearn.cluster.KMeans(n_clusters=n_components, \
                                   tol=1e-3, max_iter=50)
estimator.fit(kittens_centered)
# Plot the resulting kitten face clusters
assert n_components == n_col*n_row
plot_gallery("title_of_plot", \
             estimator.cluster_centers_[:n_components],n_col,n_row)
\end{python}
\end{adjustbox}

\vspace{6pt}

By using the following code snippet, PCA can be performed on the centered kitten data, thus generating eigenfaces, and results plotted (with the number of retained components defined as \texttt{n\char`_components} and the number of columns and rows of images to display defined as  \texttt{n\char`_col} and  \texttt{n\char`_row}, respectively). Note that, while we have already explicitly centered the data, the PCA function used here actually performs global centering for us.

\vspace{6pt}

\begin{adjustbox}{minipage=0.8\textwidth,margin=0pt \smallskipamount,center}
\begin{python}
# Compute PCA on the dataset
estimator = decomposition.RandomizedPCA(n_components=n_components, \
                                        whiten=True)
estimator.fit(kittens_centered)
# Plot the resulting PCA eigenfaces
assert n_components == n_col*n_row
plot_gallery("title_of_plot", \
             estimator.components_[:n_components],n_col,n_row)
\end{python}
\end{adjustbox}

\vspace{8pt}

\textbf{What to Provide}\\
Your output should contain the following:
\begin{itemize}
\item The face clusters computed for at least 2 values of \texttt{n\char`_components}. The number of displayed face clusters should correspond to this value.
\item The eigenfaces computed for at least 2 values of \texttt{n\char`_components}. The number of displayed eigenfaces should correspond to this value.
\end{itemize}
Given this output, respond to the following questions:
\begin{enumerate}
\item For each eigenface, darker regions indicate pixels that contribute more to that eigenface and lighter regions indicate pixels that contribute less; in this sense, the further a feature  is from a neutral color---in this case gray---the more that feature is ``captured'' by the eigenface. Briefly describe the sort of cute kitten facial features captured (i.e., those facial features represented by noticeably lighter or darker tones) by the first few eigenfaces. As the first eigenvector should capture the most variance, the first eigenface should correspond to the most general facial features.
\item How do the face clusters (generated by \textit{k}-means clustering) compare to the eigenfaces (generated by PCA)? As the kitten images have no obvious clusterings, it may be more difficult to interpret the facial features to  which each cluster corresponds.
\end{enumerate}

\end{document} 