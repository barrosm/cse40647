%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Short Sectioned Assignment
% LaTeX Template
% Version 1.0 (5/5/12)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% Original author:
% Frits Wenneker (http://www.howtotex.com)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[paper=a4, fontsize=11pt]{scrartcl} % A4 paper and 11pt font size
\usepackage[top=1in, bottom=1.5in, left=1in, right=1in]{geometry}
\usepackage{fancyhdr} % Required for custom headers
\usepackage{lastpage} % Required to determine the last page for the footer
\usepackage{extramarks} % Required for headers and footers
\usepackage[usenames,dvipsnames]{color} % Required for custom colors
\usepackage{graphicx} % Required to insert images
\usepackage{listings} % Required for insertion of code
\usepackage{courier} % Required for the courier font
\usepackage{amsmath}
\usepackage[super]{nth}
\usepackage{booktabs}
\usepackage[usenames,dvipsnames]{xcolor}
\usepackage{tcolorbox}
\usepackage{tabularx}
\usepackage{array}
\usepackage{colortbl}

%\usepackage[T1]{fontenc} % Use 8-bit encoding that has 256 glyphs
%\usepackage{fourier} % Use the Adobe Utopia font for the document - comment this line to return to the LaTeX default
\usepackage[english]{babel} % English language/hyphenation
\usepackage{amsmath,amsfonts,amsthm} % Math packages
\usepackage{graphicx}

%\usepackage{lipsum} % Used for inserting dummy 'Lorem ipsum' text into the template

\usepackage{sectsty} % Allows customizing section commands
\allsectionsfont{\centering \normalfont\scshape} % Make all sections centered, the default font and small caps
\usepackage{listings} % Required for insertion of code
\usepackage{hyperref}
\hypersetup{
  colorlinks   = true, %Colours links instead of ugly boxes
  urlcolor     = blue, %Colour for external hyperlinks
  linkcolor    = blue, %Colour of internal links
  citecolor   = red %Colour of citations
}

\usepackage{fancyhdr} % Custom headers and footers
\pagestyle{fancyplain} % Makes all pages in the document conform to the custom headers and footers
\fancyhead{} % No page header - if you want one, create it in the same way as the footers below
\fancyfoot[L]{} % Empty left footer
\fancyfoot[C]{} % Empty center footer
\fancyfoot[R]{\thepage} % Page numbering for right footer
\renewcommand{\headrulewidth}{0pt} % Remove header underlines
\renewcommand{\footrulewidth}{0pt} % Remove footer underlines
\setlength{\headheight}{13.6pt} % Customize the height of the header
\newcommand{\ts}{\textsuperscript}

\numberwithin{equation}{section} % Number equations within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)
\numberwithin{figure}{section} % Number figures within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)
\numberwithin{table}{section} % Number tables within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)

\setlength\parindent{0pt} % Removes all indentation from paragraphs - comment this line for an assignment with lots of text

% Default fixed font does not support bold face
\DeclareFixedFont{\ttb}{T1}{txtt}{bx}{n}{8} % for bold
\DeclareFixedFont{\ttm}{T1}{txtt}{m}{n}{8}  % for normal

% Custom colors
\usepackage{color}
\definecolor{mygreen}{rgb}{0,0.6,0}
\definecolor{mygray}{rgb}{0.5,0.5,0.5}
\definecolor{mymauve}{rgb}{0.58,0,0.82}

\usepackage{listings}

% Python style for highlighting
\newcommand\pythonstyle{\lstset{
language=Python,
  backgroundcolor=\color{white},   % choose the background color
  basicstyle=\footnotesize,        % size of fonts used for the code
  %breaklines=true,                 % automatic line breaking only at whitespace
  frame=tb,
  captionpos=b,                    % sets the caption-position to bottom
  commentstyle=\color{mygreen},    % comment style
  escapeinside={\%*}{*)},          % if you want to add LaTeX within your code
  keywordstyle=\color{blue},       % keyword style
  stringstyle=\color{mymauve},     % string literal style
}}


% Python environment
\lstnewenvironment{python}[1][]
{
\pythonstyle
\lstset{#1}
}
{}

% Python for external files
\newcommand\pythonexternal[2][]{{
\pythonstyle
\lstinputlisting[#1]{#2}}}

% Python for inline
\newcommand\pythoninline[1]{{\pythonstyle\lstinline!#1!}}

\usepackage{adjustbox}

%----------------------------------------------------------------------------------------
%	TITLE SECTION
%----------------------------------------------------------------------------------------

\newcommand{\horrule}[1]{\rule{\linewidth}{#1}} % Create horizontal rule command with 1 argument of height

\title{	
\normalfont \normalsize
\textsc{University of Notre Dame, Computer Science and Engineering} \\ [25pt] % Your university, school and/or department name(s)
\horrule{0.5pt} \\[0.4cm] % Thin top horizontal rule
\huge CSE 40647/60647  Data Mining \textemdash~Assignment 3\\Due Date: April \nth{4}, 2014 at 11:59pm ET\\ % The assignment title
\horrule{2pt} \\[0.5cm] % Thick bottom horizontal rule
}

\author{\textbf{Classification and Regression}} % Your name

\date{\normalsize{March 25, 2014}} % Today's date or a custom date

\begin{document}
\maketitle % Print the title

%----------------------------------------------------------------------------------------
%	PROBLEM 1
%----------------------------------------------------------------------------------------

This assignment will require you to implement and interpret some of the classification and regression concepts that were introduced in class.  \textbf{An IPython Notebook with predefined functions to assist you with this assignment is available \href{http://nbviewer.ipython.org/github/cse40647/cse40647/blob/sp.14/assignment3/assignment3.ipynb}{here}. Additionally, the dataset used in this assignment can be downloaded  \href{https://github.com/cse40647/cse40647/blob/sp.14/assignment3/paintings.zip}{here}.} Keep in mind that the main objective of this assignment is to highlight the insights that we can derive from applying these techniques---the coding aspect is secondary. Accordingly, you are welcome to consult any online documentation and/or code that has been posted to the course website, so long as all references and sources are properly cited. You are also encouraged to use code libraries, so long as you acknowledge any source code that was not written by you by mentioning the original author(s) directly in your source code (comment or header).

%\linebreak
\vspace{10pt}

\textbf{You are expected to submit a single IPython Notebook file following the same instructions and naming convention described in Assignment 0. Answers to the conceptual questions can be embedded in the Notebook as \textit{markdown} cells, and you may use \textit{heading} cells to further organize your document.}

\newpage

\section{Regression (50 points)}

\textbf{The Data}\\
For this portion of the assignment you will be using the \textit{Iris Flower dataset}, available \href{http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data}{here}.

\vspace{6pt}

The dataset consists of 50 samples from each of three species of Iris (\textit{Iris setosa}, \textit{Iris virginica} and \textit{Iris versicolor}). Four features were measured from each sample:  the length and the width of the sepals and petals, in centimeters.

\vspace{8pt}

\textbf{The Idea:  Using Linear Regression on the Iris Dataset}\\
Often, we may want to predict one feature based upon other features. Your objective here will be to generate a linear model of one of the features (a continuous variable) in the Iris dataset using one or more of the remaining features and/or class values. In our case, we're interested in finding the best linear model among those that can be generated from this dataset.

\vspace{8pt}

\textbf{What to Do}\\
First, load the Iris dataset. This can be done using the following snippet of code:

\vspace{6pt}

\begin{adjustbox}{minipage=0.8\textwidth,margin=0pt \smallskipamount,center}
\begin{python}
import pandas as pd
fileURL = 'http://archive.ics.uci.edu/ml/\
machine-learning-databases/iris/iris.data'
iris = pd.read_csv(fileURL, names=['Sepal_Length', 'Sepal_Width', \
                                   'Petal_Length', 'Petal_Width',
                                   'Species'], header=None)
iris = iris.dropna()
\end{python}
\end{adjustbox}

\vspace{6pt}

Next, you can visualize the correlation between different features using the following snippet of code, which executes the provided \texttt{pairs} function:

\vspace{6pt}

\begin{adjustbox}{minipage=0.8\textwidth,margin=0pt \smallskipamount,center}
\begin{python}
pairs(iris)
\end{python}
\end{adjustbox}

\vspace{6pt}

Some pairs of features tend to be more correlated than others. Try to uncover related features by using linear regression to model the relationship between pairs of features. In other words, use one feature as a target (dependent) variable and another feature as a predictor (independent) variable. To generate a linear regression model, you may use the \\ \texttt{linear\char`_model.LinearRegression()} function available via the scikit-learn library. To run the model on the Iris data, first divide the dataset into training and testing sets, then fit the model on the training set and predict with the fitted model on the testing set. scikit-learn provides several functions for dividing datasets in this manner, including \\ \texttt{cross\char`_validation.KFold} and \texttt{cross\char`_validation.train\char`_test\char`_split}.

\vspace{6pt}

Several statistics can be generated from a linear model. Given a fitted linear model, the following code outputs the model coefficients (the parameter values for the fitted model), the residual sum of squares (the model error), and the explained variance (the degree to which the model explains the variation present in the data):

\vspace{6pt}

\begin{adjustbox}{minipage=0.8\textwidth,margin=0pt \smallskipamount,center}
\begin{python}
# The coefficients
print "Coefficients: \n", regr.coef_
# The mean square error
print ("Residual sum of squares: %.2f" %
        np.mean((regr.predict(iris_X_test) - iris_y_test) ** 2))
# Explained variance score (1 is perfect prediction)
print ("Variance score: %.2f" % regr.score(iris_X_test, iris_y_test))
\end{python}
\end{adjustbox}

\vspace{6pt}

You can use these scores to measure the efficacy of a particular linear model.

\vspace{8pt}

\textbf{What to Provide}\\
Your output should contain the following:
\begin{itemize}
\item A scatterplot matrix of scatterplots, with one scatterplot for each pairwise combination of features.
\item A plot of the linear regression models generated on each pairwise combination of features, with corresponding model statistics.
\item A plot of the best overall linear regression model you were able to generate using any combination of features (including the use of multiple features used in combination to predict a single feature), with corresponding model statistics.
\end{itemize}
Given this output, respond to the following questions:
\begin{enumerate}
\item Based upon the linear models you generated, which pair of features appear to be most predictive for one another? Note that you can answer this question based upon the output provided for the linear models.
\item Suppose you tried to generate a classification model on this dataset, but only after removing the feature that you were best able to predict based upon other features. How would removing this feature affect the classification performance?
\end{enumerate}

\newpage

\section{Classification (50 points)}

\textbf{The Data}\\
For this portion of the assignment you will be using a set of images of the works of various artists. To acquire the large number of images, we used a script that searches for each artist's painting on Google Images and downloads it. The provided dataset consists of images obtained in this manner for several well-known artists. The image are of varying size and quality.

\vspace{6pt}

The provided dataset is available \href{https://github.com/cse40647/cse40647/blob/sp.14/assignment3/paintings.zip}{here}.

\vspace{6pt}

You are welcome to use the included function (\texttt{go\char`_google\char`_image}) to search and retrieve paintings by your preferred artist(s), which may then be used for this assignment.

\vspace{8pt}

\textbf{The Idea:  Identifying Artists by their Paintings}\\
Your objective here will be to perform classification on the dataset to discern how reliably the paintings can be attributed to their respective artists. Several data preprocessing steps, including dimensionality reduction and clustering, may be applied to generate features more amenable to this task. Specifically, we would like to perform classification on the images so that they are correctly attributed to this respective artists. We can assess classification performance by dividing the dataset into a training set and a testing set, and measuring the error of a classifier fitted on the training set and evaluated on the testing set.

\vspace{6pt}

Intuitively, since we typically view an image as a collection of pixels, we might consider using the set of pixels as a feature. In other words, each color value for each pixel would be a feature, with the collection of pixels comprising many features that collectively describe the pixel values of the image. We call the collection of features that describe a particular aspect of the image a ``feature descriptor.'' While this feature descriptor describes each pixel of the image, the specific locations of particular color values and the size of the image directly affect the derived feature values, thus making it difficult to use this set of features to directly compare images.

\vspace{6pt}

We can supplement or replace the pixel-based features with a histogram of color values. Each color has 256 possible values, resulting in a histogram of 768 (256*3) color values distributed over the entire range of pixels. Each value of the histogram is the number of pixels in the image with the corresponding color value. Here, we would consider each histogram value as a feature of an image. The histogram provides a representation of the color distribution of an image, ignoring the specific location of color values within the image. Each value of the histogram then corresponds to one feature.

\vspace{6pt}

We can also use more complicated features, such as histograms of Oriented Gradients (HOGs). HOGs are feature descriptors used in computer vision and image processing for the purpose of object detection that count the occurrences of gradient orientation in localized portions of an image. HOG descriptors are based upon the premise that local object appearance and shape within an image can be described by the distribution of intensity gradients or edge directions. These properties can derived by dividing the image into small connected regions or ``cells,'' and compiling a histogram of gradient directions or edge orientations for the pixels within each cell. The combination of these histograms then represents the HOG descriptor, which has been shown to be quite effective for the purposes of classifying images.

\vspace{6pt}

While these are several of the popular feature descriptors used for image classification, other feature descriptors could be incorporated in the final set of feature used to train a classification model. In addition, each of these feature descriptors could be further preprocessed for further enhance their descriptive power.

\vspace{8pt}

\textbf{What to Do}\\
The IPython Notebook provided with this assignment includes functions to compute the histograms and plot the images within the transformed (2-dimensional) space (\texttt{load\char`_images} and \texttt{plot\char`_image\char`_space}, respectively). There are also functions to generate and plot the color palettes associated with each image (\texttt{cluster\char`_image\char`_colors} and \texttt{plot\char`_color\char`_palette}, respectively); the palettes are generated via (\textit{k}-means) clustering of the pixel color values, and may be investigated at your own leisure---they are not needed to complete the assignment.

\vspace{6pt}

The images can be loaded and the histograms generated by running the following code snippet (which imports pandas as \texttt{pd}). Please ensure that the directory provided to the \texttt{load\char`_images} function is correct. For example, if you have placed all the images in your base IPython Notebook directory in a folder labeled \texttt{images}, with the paintings images in a subfolder labeled \texttt{paintings}, then the (relative) path to the painting images would be \texttt{images/paintings}. The following code snippet loads all images from the `Pablo Picasso Paintings' and `Vincent van Gogh Paintings' subdirectories (the \texttt{painters\char`_subdirs} variable can be changed to your painters of interest, or omitted to load all of the subdirectories) and generates feature descriptors based upon the loaded images:

\vspace{6pt}

\begin{adjustbox}{minipage=0.8\textwidth,margin=0pt \smallskipamount,center}
\begin{python}
import pandas as pd
# Load images and generate initial feature descriptors
painters_dir = '/path/to/painting_images' # directory path
painters_subdirs = [u'Pablo Picasso Paintings',
                    u'Vincent van Gogh Paintings']
data = fetch_paintings(painters_dir, painters_subdirs)
gen_pixel_fd(data) # generate pixel-based feature descriptor
gen_hist_fd(data) # generate color histogram feature descriptor
gen_hog_fd(data) # generate HOG feature descriptor
\end{python}
\end{adjustbox}

\vspace{6pt}

Each of these feature descriptors has a different representational format. However, the classification models available via scikit-learn only accept data into a single-dimensional vector format. Accordingly, these features need to be flattened into a single long array. The following code snippet accomplishes these tasks:

\vspace{6pt}

\begin{adjustbox}{minipage=0.8\textwidth,margin=0pt \smallskipamount,center}
\begin{python}
# Arrange feature descriptors into a single feature vector
images = [pp['image'] for p in data.keys() for pp in data[p]]
X = pd.DataFrame([pp['features'] \
               for p in data.keys() for pp in data[p]])
y = pd.DataFrame([cls for cls in [pp['class'] \
               for p in data.keys() for pp in data[p]]])
for index in pd.isnull(X).any(1).nonzero()[0]: del images[index]
y = y.drop(y.index[pd.isnull(X).any(1).nonzero()[0]])
X = X.dropna()
\end{python}
\end{adjustbox}

\vspace{6pt}

Given this feature vector, visualize the data into two-dimensions using a method of your choice. Subsequently classify the data using 5-fold cross-validation, using one or more classifiers or your choice.

\vspace{8pt}

\textbf{What to Provide}\\
Your output should contain the following:
\begin{itemize}
\item For a pair of artists of your choice, generate a two-dimensional visualization.
\item For a pair of artists of your choice, generate at least three classification models that distinguish between the artists' paintings.
\item Generate at least one feature descriptor (in addition to those provided) and include it in the set of features used for your classifiers. Rerun the classifiers with the newly-included feature descriptor.
\end{itemize}
Given this output, respond to the following questions:
\begin{enumerate}
\item What is the highest classification accuracy you achieve?
\item Of the classification models you tried, did any tend to perform better or worse than others? Any thoughts why?
\item What feature descriptor(s) did you generate? Did they improve the classification performance? If so, by how much?
\end{enumerate}

\vspace{8pt}

\section{Extra Credit Portion (+10 points)}
\textbf{The Idea:  Good Classifiers Generalize Well}\\
A good classifier should generalize well to data it has not yet seen. When performing any classification task, the typical objective is to minimize the generalization error that a classification model will produce on new data.

\vspace{8pt}

\textbf{What to Do}\\
Generate the best classifier you can for distinguishing between two different artists' paintings, per the classification problem outlined above. We define the best-performing model here as the model with the highest accuracy. We will use a new dataset from two unprovided artists to test each student's model using 5-fold cross-validation. The student with the best-performing model (the model with the highest accuracy on this new dataset) will earn 10 points extra credit; the student with the second-best model will earn 9 points; and so on, with the tenth-best model earning 1 point. Thus up to ten students will receive some degree of extra credit on this task.

\vspace{8pt}

\textbf{What to Provide}\\
If you're interested in participating, ensure that your model is properly delineated in and runnable from your notebook. All content needed to generate your classification model should be included in your assignment submission.
\end{document} 